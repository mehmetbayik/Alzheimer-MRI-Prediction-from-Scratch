{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "dependencies:\n",
    "  - python=3.8.17\n",
    "  - numpy=1.24.0\n",
    "  - matplotlib=3.7.1\n",
    "  - pandas=2.0.2 \n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from itertools import product \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# import random\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# np.random.RandomState(42)\n",
    "# os.environ['TF_DETERMINISTIC_OPS'] = '1' \n",
    "\n",
    "finish_sound = \"afplay /Users/mehmet/Documents/vs-code/winsquare.mp3\"\n",
    "# play sound when finished\n",
    "# os.system(finish_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5120, 10859) (5120, 4) \n",
      " (640, 10859) (640, 4) \n",
      " (640, 10859) (640, 4)\n"
     ]
    }
   ],
   "source": [
    "# Read data from npy file ( already preprocessed )\n",
    "filename = 'original-numpy'\n",
    "# filename = 'pca-numpy'\n",
    "X_train = np.load(f'dataset/{filename}/X_train.npy')\n",
    "X_val = np.load(f'dataset/{filename}/X_val.npy')\n",
    "X_test = np.load(f'dataset/{filename}/X_test.npy')\n",
    "y_train = np.load(f'dataset/{filename}/y_train.npy')\n",
    "y_val = np.load(f'dataset/{filename}/y_val.npy')\n",
    "y_test = np.load(f'dataset/{filename}/y_test.npy')\n",
    "\n",
    "# # Add Bias to X\n",
    "# X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)\n",
    "# X_val = np.concatenate((np.ones((X_val.shape[0], 1)), X_val), axis=1)\n",
    "# X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "\n",
    "# # Remove one hot encoding from y\n",
    "# y_train = np.argmax(y_train, axis=1)\n",
    "# y_val = np.argmax(y_val, axis=1)\n",
    "# y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(X_train.shape, y_train.shape,'\\n', X_val.shape, y_val.shape,'\\n', X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.W = None\n",
    "        self.now = None\n",
    "        self.print_result = True\n",
    "        self.history_steps1 = None\n",
    "        self.history = None\n",
    "        self.validation_accuracy = None\n",
    "        \n",
    "    def validation_accuracy(self):\n",
    "        return self.validation_accuracy\n",
    "    \n",
    "    def history(self):\n",
    "        return self.history\n",
    "    \n",
    "    def load_history(self):\n",
    "        pd_hist = pd.read_csv(f'model-comparison/{self.now}/history.csv')\n",
    "        self.history = np.array(pd_hist.iloc[:,1:])\n",
    "        \n",
    "    def plot(self, save = True):\n",
    "        # Save history as csv file\n",
    "        history_local = self.history\n",
    "        if type(history_local) is not pd.DataFrame:\n",
    "            history_df = pd.DataFrame(history_local)\n",
    "        if save == True:\n",
    "            hist_csv_file = f'model-comparison/{self.now}/history.csv'\n",
    "            with open(hist_csv_file, mode='w') as f:\n",
    "                history_df.to_csv(f) \n",
    "        # Plot Loss and Accuracy History as Subplots\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(10, 2)\n",
    "        index = np.arange(1,self.history.shape[1]+1)*self.history_steps1\n",
    "\n",
    "        ax[0].plot(index, self.history[0], label='Training Loss')\n",
    "        ax[0].plot(index, self.history[2], label='Validation Loss')\n",
    "        ax[0].set_title('Loss History')\n",
    "        ax[0].set_xlabel('Epoch')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].legend()\n",
    "\n",
    "        ax[1].plot(index, self.history[1], label='Training Accuracy')\n",
    "        ax[1].plot(index, self.history[3], label='Validation Accuracy')\n",
    "        ax[1].set_title('Accuracy History')\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].set_ylabel('Accuracy')\n",
    "        ax[1].legend()\n",
    "        if save is True and self.now is not None:\n",
    "            plt.savefig(f'model-comparison/{self.now}/plot.png')\n",
    "        if self.print_result == True:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "        \n",
    "\n",
    "    def loss(self, X_nonbiased, y, W):\n",
    "        # Hinge loss\n",
    "        if X_nonbiased.shape[1] != W.shape[0]:\n",
    "            ones=np.ones(X_nonbiased.shape[0])\n",
    "            X=np.c_[ones,X_nonbiased]\n",
    "        else:\n",
    "            X = X_nonbiased\n",
    "        scores = X.dot(W)\n",
    "        num_samples = X.shape[0]\n",
    "        correct_class_mask = (np.arange(num_samples), y)\n",
    "        margins = np.maximum(0, scores - scores[correct_class_mask][:, np.newaxis] + 1)\n",
    "        margins[correct_class_mask] = 0\n",
    "        loss = np.sum(margins)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def gradient(self, X, y, W):\n",
    "        scores = X.dot(W)\n",
    "        num_samples = X.shape[0]\n",
    "        correct_class_mask = (np.arange(num_samples), y)\n",
    "        margins = np.maximum(0, scores - scores[correct_class_mask][:, np.newaxis] + 1)\n",
    "        margins[correct_class_mask] = 0\n",
    "        grad_mask = (margins > 0).astype(float)\n",
    "        grad_mask[correct_class_mask] = -np.sum(grad_mask, axis=1)\n",
    "        return grad_mask\n",
    "        \n",
    "    def fit(self, X_nonbiased, y, X_val, y_val, now=None, print_result = True,\n",
    "            batch_size=5120, weight_init='zero', lr=0.01, lr_type = 'static', lmbda=0.01, max_epoch=1000,\n",
    "            history_steps = 50, print_step = 100):\n",
    "        start_time = datetime.datetime.now()\n",
    "        # if there isn't model-comparison folder, create it\n",
    "        if not os.path.exists('model-comparison'):\n",
    "            os.mkdir('model-comparison')\n",
    "        self.print_result = print_result\n",
    "        if now is not None:\n",
    "            self.now = now\n",
    "        # Create folder for current model\n",
    "            if not os.path.exists('model-comparison/'+now):\n",
    "                os.mkdir('model-comparison/'+now)\n",
    "\n",
    "        self.history_steps1 = history_steps\n",
    "        self.history = np.zeros((4,max_epoch//history_steps))\n",
    "        y_onehot = y\n",
    "        lr_print = str(lr) + ' ' + lr_type\n",
    "        model_specs = 'SVM | Batch Size: {} | Weight Init. {} | lr: {} | Lambda: {} | Max Epoch: {} |'.format(batch_size, weight_init, lr_print, lmbda, max_epoch)\n",
    "\n",
    "        # add bias\n",
    "        ones=np.ones(X_nonbiased.shape[0])\n",
    "        X=np.c_[ones,X_nonbiased]\n",
    "        \n",
    "        num_classes = y.shape[1]\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        #self.W = np.random.rand(num_features, num_classes)\n",
    "        \n",
    "        # zero initialization\n",
    "        # bias included in W\n",
    "        self.W = np.zeros((num_features, num_classes))\n",
    "\n",
    "        # One hot encoded to not one hot encoded\n",
    "        y = np.argmax(y, axis=1)\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "        \n",
    "        # Print loss and accuracy every 100 iterations or every max_iter//10 iterations if max_iter >= 1000\n",
    "        if max_epoch >= 1000:\n",
    "            print_step = max_epoch // 10\n",
    "\n",
    "        # Gradient Descent\n",
    "        for epoch in range(1,max_epoch+1):\n",
    "            \n",
    "            # Shuffle all data X and y in the same order every epoch\n",
    "            shuffle_index = np.arange(X.shape[0])\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            X = X[shuffle_index]\n",
    "            y = y[shuffle_index]\n",
    "            \n",
    "            for iteration in range(X.shape[0]//batch_size):          \n",
    "                                              \n",
    "                X_batch = X[batch_size*iteration:batch_size*(iteration+1)]\n",
    "                y_batch = y[batch_size*iteration:batch_size*(iteration+1)]\n",
    "            \n",
    "                reg_term = lmbda * self.W\n",
    "                reg_term[0] = 0\n",
    "                \n",
    "            \n",
    "                gradient = self.gradient(X_batch, y_batch, self.W)\n",
    "                \n",
    "                if lr_type[0:8] == 'momentum':\n",
    "                    if epoch == 1:\n",
    "                        last_gradient = gradient\n",
    "                    else:\n",
    "                        momentum = float(lr_type[10:])\n",
    "                        gradient = gradient + momentum * last_gradient\n",
    "                        last_gradient = gradient\n",
    "                \n",
    "                \n",
    "                self.W -= lr * (X_batch.T.dot(gradient) / batch_size + reg_term)\n",
    "                \n",
    "            if lr_type[0:8] == 'adaptive' and epoch % 500 == 0:\n",
    "                    k = float(lr_type[9:])\n",
    "                    lr *= k  \n",
    "\n",
    "            # For each 100 epochs print losses and accuracy\n",
    "            if epoch % history_steps == 0:\n",
    "                # how to calculate accuracy\n",
    "                loss = self.loss(X, y, self.W)\n",
    "                val_loss = self.loss(X_val, y_val, self.W)\n",
    "                accuracy = np.mean(self.predict(X) == y)  \n",
    "                val_acc = np.mean(self.predict(X_val) == y_val)   \n",
    "                self.validation_accuracy = val_acc\n",
    "                self.history[:,(epoch//history_steps)-1] = np.array([loss, accuracy, val_loss, val_acc])\n",
    "                \n",
    "                if epoch % print_step == 0:\n",
    "                    line1 = 'Epoch: ' + str(epoch)\n",
    "                    line2 = ' | Loss: ' + str(loss)[:5] + ' | Accuracy: ' + str(accuracy)[0:5]\n",
    "                    line3 = ' | Val. Loss: ' + str(val_loss)[:5] + ' | Val. Acc: ' + str(val_acc)[0:5]\n",
    "                    # line2 = ' | Loss: ' + str(round(loss)) + ' | Accuracy: ' + str(accuracy)[0:5]\n",
    "                    # line3 = ' | Val. Loss: ' + str(round(val_loss)) + ' | Val. Acc: ' + str(val_acc)[0:5]\n",
    "                    if print_result == True:\n",
    "                        print(line1 + line2 + line3)\n",
    "                    if now is not None:\n",
    "                        with open('model-comparison/{}/log.txt'.format(now), 'a') as f:\n",
    "                            f.write(line1 + line2 + line3 + '\\n')\n",
    "                            \n",
    "            if epoch == max_epoch:\n",
    "                end_time = datetime.datetime.now()\n",
    "                if print_result == True:\n",
    "                    print('Training finished. Time elapsed:', end_time - start_time, '\\n')\n",
    "                    print('Accuracy: ', str(accuracy)[0:5], 'Val. Accuracy: ', str(val_acc)[0:5])\n",
    "                val_acc_print = str(val_acc*100)+ '00'\n",
    "                if now is not None:\n",
    "                    with open('model-comparison/{}/log.txt'.format(now), 'a') as f:\n",
    "                        write_line = 'Training finished. Time elapsed: ' + str(end_time - start_time) + '\\n'\n",
    "                        f.write(write_line)\n",
    "                    with open('model-comparison/{}/{}-val-acc.txt'.format(now,val_acc_print[0:5]), 'w') as f:\n",
    "                        f.write(model_specs)\n",
    "                    with open('model-comparison/last.txt', 'w') as f:\n",
    "                        f.write(str(now))\n",
    "                            \n",
    "    def predict(self, X_nonbiased):\n",
    "        if X_nonbiased.shape[1] != self.W.shape[0]: \n",
    "            # add bias\n",
    "            ones=np.ones(X_nonbiased.shape[0])\n",
    "            X=np.c_[ones,X_nonbiased]\n",
    "        else:\n",
    "            X = X_nonbiased\n",
    "        \n",
    "        scores = X.dot(self.W)\n",
    "        predictions = np.argmax(scores, axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    def save_weights(self):\n",
    "        filename = 'model-comparison/{}/weights.npy'.format(self.now)\n",
    "        np.save(filename, self.W)\n",
    "    def load_weights(self, now):\n",
    "        filename = 'model-comparison/{}/weights.npy'.format(now)\n",
    "        self.W = np.load(filename)\n",
    "        self.now = now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateModel():\n",
    "    # Class to evaluate model performance, similar to sklearn.metrics ClassificationReport and ConfusionMatrix\n",
    "    def __init__(self, y_true, y_pred, str1, now, save=True, print_result=True):\n",
    "        self.y_true = np.argmax(y_true, axis=1)\n",
    "        self.y_pred = y_pred\n",
    "        if save == True:\n",
    "            os.mkdir('model-comparison/'+now+'/'+str1)\n",
    "            np.savetxt('model-comparison/{}/{}/pred.csv'.format(now,str1), y_pred, delimiter=',', fmt='%d')\n",
    "        \n",
    "        result = self.classification_report()\n",
    "        fpr0 = 100 - float(result['precision'][0][0:4])\n",
    "        line1 = 'Accuracy is: ' + str(result['f1-score']['accuracy'])\n",
    "        line2 = 'F1 Score is: ' + str(result['f1-score']['weighted avg'])\n",
    "        line3 = 'Precision of Class 0 is: ' + '{0:.2f}'.format(100-fpr0)+ ' %'\n",
    "        line4 = '\\nClassification Report:'\n",
    "        line5 = '\\nConfusion Matrix:'\n",
    "        cm = self.confusion_matrix()\n",
    "        line6 = '\\n'\n",
    "        res_total = line1 + '\\n' + line2 + '\\n' + line3 + '\\n' + line4 + '\\n' + str(result) + '\\n' + line5 + '\\n' + str(cm) + '\\n' + line6\n",
    "        # write to file\n",
    "        if save == True:\n",
    "            with open('model-comparison/{}/{}/report.txt'.format(now,str1), 'w') as f:\n",
    "                f.write(res_total)\n",
    "        if print_result == True:\n",
    "            print(res_total)\n",
    "\n",
    "    def accuracy_score(self, y_t, y_p):\n",
    "        correct = sum(y_t == y_p)\n",
    "        return correct / len(y_t)\n",
    "\n",
    "    def scores(self, y_t, y_p, class_label= 1):\n",
    "        true = y_t == class_label\n",
    "        pred = y_p == class_label\n",
    "        tp = sum(true & pred)\n",
    "        fp = sum(~true & pred) \n",
    "        fn = sum(true & ~pred)\n",
    "        tn = sum(~true & ~pred) \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return precision, recall, f1\n",
    "    \n",
    "    def confusion_matrix(self,labels=None):\n",
    "        labels = labels if labels else sorted(set(self.y_true) | set(self.y_pred))        \n",
    "        indexes = {v:i for i, v in enumerate(labels)}\n",
    "        matrix = np.zeros((len(indexes),len(indexes))).astype(int)\n",
    "        for t, p in zip(self.y_true, self.y_pred):\n",
    "            matrix[indexes[t], indexes[p]] += 1\n",
    "        # print('Confusion Matrix: ')\n",
    "        # print(pd.DataFrame(matrix, index=labels, columns=labels))\n",
    "        return pd.DataFrame(matrix, index=labels, columns=labels)\n",
    "\n",
    "    def classification_report(self):\n",
    "        output_dict = {}\n",
    "        support_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        f1_list = []\n",
    "        for i in np.unique(self.y_true):\n",
    "            support = sum(self.y_true == i)\n",
    "            precision, recall, f1 = self.scores(self.y_true, self.y_pred, class_label=i)\n",
    "            output_dict[i] = {'precision':precision, 'recall':recall, 'f1-score':f1, 'support':support}\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            f1_list.append(f1)\n",
    "            support_list.append(support)\n",
    "        support = np.sum(support_list)\n",
    "        output_dict['accuracy'] = {'precision':0, 'recall':0, 'f1-score':self.accuracy_score(self.y_true, self.y_pred), 'support':support}\n",
    "        # macro avg\n",
    "        macro_precision = np.mean(precision_list)\n",
    "        macro_recall = np.mean(recall_list)\n",
    "        macro_f1 = np.mean(f1_list)\n",
    "        output_dict['macro avg'] = {'precision':macro_precision, 'recall':macro_recall, 'f1-score':macro_f1, 'support':support}\n",
    "        # weighted avg\n",
    "        weighted_precision = np.average(precision_list, weights=support_list)\n",
    "        weighted_recall = np.average(recall_list, weights=support_list)\n",
    "        weighted_f1 = np.average(f1_list, weights=support_list)\n",
    "        output_dict['weighted avg'] = {'precision':weighted_precision, 'recall':weighted_recall, 'f1-score':weighted_f1, 'support':support}\n",
    "        # convert to dataframe and format\n",
    "        report_d = pd.DataFrame(output_dict).T\n",
    "        annot = report_d.copy()\n",
    "        annot.iloc[:, 0:3] = (annot.iloc[:, 0:3]*100).applymap('{:.2f}'.format) + ' %'\n",
    "        annot['support'] = annot['support'].astype(int)\n",
    "        annot.loc['accuracy','precision'] = ''\n",
    "        annot.loc['accuracy','recall'] = ''\n",
    "        return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(model_options, X_train, y_train, X_val, y_val, X_test, y_test, print_result=False, seed=42):\n",
    "    # Grid Search Function\n",
    "    best_metric = 0\n",
    "    for i in range(len(model_options)):\n",
    "        models = model_options[i]\n",
    "        model_number = i + 1\n",
    "        now = datetime.datetime.now().strftime(\"%d-%m-%H-%M\")\n",
    "        # Create folder for current model\n",
    "        if not os.path.exists('model-comparison/'+now):\n",
    "            os.mkdir('model-comparison/'+now)\n",
    "        else:\n",
    "            now = now + str('--1')\n",
    "            os.mkdir('model-comparison/'+now)\n",
    "        model = SVM(seed=seed)\n",
    "        start_time = datetime.datetime.now()\n",
    "        model.fit(X_train, y_train, X_val, y_val, now, print_result=print_result, max_epoch=models[0], \n",
    "                  weight_init= models[1], batch_size=models[2], lr=models[3], lr_type=models[4], lmbda=models[5])\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_elapsed = str(end_time - start_time)[2:7]\n",
    "        metric = model.validation_accuracy\n",
    "        model.save_weights()\n",
    "        model.plot()\n",
    "        y_pred = model.predict(X_val)\n",
    "        results = EvaluateModel(y_val, y_pred, 'val', now, print_result=print_result)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results = EvaluateModel(y_test, y_pred, 'test', now, print_result=print_result)\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            best_model = now\n",
    "        print('Model ', str(model_number), ' saved with name: ', now)\n",
    "        print(models, 'Val-Accuracy:', metric)\n",
    "\n",
    "        # append to txt file\n",
    "        lr_print = str(models[3]) + ' ' + models[4]\n",
    "        model_specs = 'SVM | Batch Size: {} | Weight Init: {} | lr: {} | Lambda: {} | Max Epoch: {}'.format(models[2], models[1], lr_print, models[5], models[0])\n",
    "        with open('model-comparison/best-models.txt', 'a') as f:\n",
    "            f.write(now + ' | ' + model_specs + ' | ' + str(metric) + ' | Time Elapsed: '+ time_elapsed +'\\n')\n",
    "        print(len(model_options)-model_number, 'models left to train.')\n",
    "    best_metric = str(best_metric*100)[:5]\n",
    "    print('Best Model is:', best_model, 'with validation accuracy:', best_metric, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(max_epoch, batch_size, weight_init, lr, lr_type, lmbda):\n",
    "    model_options = [[max_epoch, weight_init, batch_size, lr, lr_type, lmbda]]\n",
    "    return model_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200 | Loss: 1623. | Accuracy: 0.941 | Val. Loss: 524.5 | Val. Acc: 0.828\n",
      "Epoch: 400 | Loss: 604.1 | Accuracy: 0.977 | Val. Loss: 399.8 | Val. Acc: 0.864\n",
      "Epoch: 600 | Loss: 406.5 | Accuracy: 0.984 | Val. Loss: 369.9 | Val. Acc: 0.871\n",
      "Epoch: 800 | Loss: 381.4 | Accuracy: 0.985 | Val. Loss: 366.7 | Val. Acc: 0.873\n",
      "Epoch: 1000 | Loss: 358.8 | Accuracy: 0.986 | Val. Loss: 363.5 | Val. Acc: 0.876\n"
     ]
    }
   ],
   "source": [
    "# Train New Model\n",
    "model_parameters = TrainModel(\n",
    "    max_epoch=2000, batch_size=512, weight_init='zero', lr=0.0001, lr_type='adaptive 0.1', lmbda=0.001)\n",
    "\n",
    "GridSearch(model_parameters, X_train, y_train, X_val, y_val, X_test, y_test, print_result=True, seed=42)\n",
    "\n",
    "os.system(finish_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train New Model\n",
    "# now = datetime.datetime.now().strftime(\"%d-%m-%H-%M\")\n",
    "\n",
    "# model = SVM(seed=42)\n",
    "# model.fit(X_train, y_train, X_val, y_val, now=now, print_result=True,\n",
    "#         lr=0.001, lmbda=0.001, max_epoch=1000)\n",
    "\n",
    "# model.save_weights()\n",
    "# model.plot()\n",
    "\n",
    "# # Validation Set Results\n",
    "# y_pred = model.predict(X_val)\n",
    "# results = EvaluateModel(y_val, y_pred, 'val', now)\n",
    "\n",
    "# # Test Set Results\n",
    "# y_pred = model.predict(X_test)\n",
    "# results = EvaluateModel(y_test, y_pred, 'test', now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations: 12\n",
      "Combination 1: (1000, 'zero', 5120, 0.01, 'static', 0.01)\n"
     ]
    }
   ],
   "source": [
    "# Grid Search Combinations\n",
    "\n",
    "max_epoch = [1000]\n",
    "weight_init = ['zero']\n",
    "#batch_size = [1, 512, 5120]\n",
    "batch_size = [5120]\n",
    "# lr = [0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "lr = [0.01, 0.001, 0.0001]\n",
    "\n",
    "lr_type = ['static']\n",
    "regularization = [0.01, 0.001, 0.0001, 0]\n",
    "#regularization = [0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "params = [max_epoch, weight_init, batch_size, lr, lr_type, regularization]\n",
    "model_options = list(product(*params))\n",
    "print('Number of combinations:', len(model_options))\n",
    "print('Combination 1:', model_options[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  1  saved with name:  18-12-17-34\n",
      "(1000, 'zero', 5120, 0.01, 'static', 0.01) Val-Accuracy: 0.8546875\n",
      "11 models left to train.\n",
      "Model  2  saved with name:  18-12-17-35\n",
      "(1000, 'zero', 5120, 0.01, 'static', 0.001) Val-Accuracy: 0.8515625\n",
      "10 models left to train.\n",
      "Model  3  saved with name:  18-12-17-37\n",
      "(1000, 'zero', 5120, 0.01, 'static', 0.0001) Val-Accuracy: 0.8515625\n",
      "9 models left to train.\n",
      "Model  4  saved with name:  18-12-17-38\n",
      "(1000, 'zero', 5120, 0.01, 'static', 0) Val-Accuracy: 0.85\n",
      "8 models left to train.\n",
      "Model  5  saved with name:  18-12-17-40\n",
      "(1000, 'zero', 5120, 0.001, 'static', 0.01) Val-Accuracy: 0.89375\n",
      "7 models left to train.\n",
      "Model  6  saved with name:  18-12-17-41\n",
      "(1000, 'zero', 5120, 0.001, 'static', 0.001) Val-Accuracy: 0.9\n",
      "6 models left to train.\n",
      "Model  7  saved with name:  18-12-17-43\n",
      "(1000, 'zero', 5120, 0.001, 'static', 0.0001) Val-Accuracy: 0.9\n",
      "5 models left to train.\n",
      "Model  8  saved with name:  18-12-17-44\n",
      "(1000, 'zero', 5120, 0.001, 'static', 0) Val-Accuracy: 0.89375\n",
      "4 models left to train.\n",
      "Model  9  saved with name:  18-12-17-46\n",
      "(1000, 'zero', 5120, 0.0001, 'static', 0.01) Val-Accuracy: 0.7625\n",
      "3 models left to train.\n",
      "Model  10  saved with name:  18-12-17-47\n",
      "(1000, 'zero', 5120, 0.0001, 'static', 0.001) Val-Accuracy: 0.7625\n",
      "2 models left to train.\n",
      "Model  11  saved with name:  18-12-17-48\n",
      "(1000, 'zero', 5120, 0.0001, 'static', 0.0001) Val-Accuracy: 0.7625\n",
      "1 models left to train.\n",
      "Model  12  saved with name:  18-12-17-50\n",
      "(1000, 'zero', 5120, 0.0001, 'static', 0) Val-Accuracy: 0.7625\n",
      "0 models left to train.\n",
      "Best Model is: 18-12-17-41 with validation accuracy: 90.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search All Combinations\n",
    "\n",
    "GridSearch(model_options, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "os.system(finish_sound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs464",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
