{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! jupyter nbconvert --to html dataset-preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\ndependencies:\\n  - python=3.8.17\\n  - numpy=1.24.0\\n  - matplotlib=3.7.1\\n  - pandas=2.0.2 \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "dependencies:\n",
    "  - python=3.8.17\n",
    "  - numpy=1.24.0\n",
    "  - matplotlib=3.7.1\n",
    "  - pandas=2.0.2 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from itertools import product \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "# import random\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# np.random.RandomState(42)\n",
    "# os.environ['TF_DETERMINISTIC_OPS'] = '1' \n",
    "\n",
    "finish_sound = \"afplay /Users/mehmet/Documents/vs-code/winsquare.mp3\"\n",
    "# play sound when finished\n",
    "# os.system(finish_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5120, 10859) (5120,) \n",
      " (640, 10859) (640,) \n",
      " (640, 10859) (640,)\n"
     ]
    }
   ],
   "source": [
    "# Read data from npy file ( already preprocessed )\n",
    "filename = 'original-numpy'\n",
    "# filename = 'pca-numpy'\n",
    "X_train = np.load(f'dataset/{filename}/X_train.npy')\n",
    "X_val = np.load(f'dataset/{filename}/X_val.npy')\n",
    "X_test = np.load(f'dataset/{filename}/X_test.npy')\n",
    "y_train = np.load(f'dataset/{filename}/y_train.npy')\n",
    "y_val = np.load(f'dataset/{filename}/y_val.npy')\n",
    "y_test = np.load(f'dataset/{filename}/y_test.npy')\n",
    "\n",
    "# Remove one hot encoding from y\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(X_train.shape, y_train.shape,'\\n', X_val.shape, y_val.shape,'\\n', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateModel():\n",
    "    # Class to evaluate model performance, similar to sklearn.metrics ClassificationReport and ConfusionMatrix\n",
    "    def __init__(self, y_true, y_pred, str1, now, save=True, print_result=True):\n",
    "        self.y_true = np.argmax(y_true, axis=1)\n",
    "        self.y_pred = y_pred\n",
    "        if save == True:\n",
    "            os.mkdir('model-comparison/'+now+'/'+str1)\n",
    "            np.savetxt('model-comparison/{}/{}/pred.csv'.format(now,str1), y_pred, delimiter=',', fmt='%d')\n",
    "        \n",
    "        result = self.classification_report()\n",
    "        fpr0 = 100 - float(result['precision'][0][0:4])\n",
    "        line1 = 'Accuracy is: ' + str(result['f1-score']['accuracy'])\n",
    "        line2 = 'F1 Score is: ' + str(result['f1-score']['weighted avg'])\n",
    "        line3 = 'Precision of Class 0 is: ' + '{0:.2f}'.format(100-fpr0)+ ' %'\n",
    "        line4 = '\\nClassification Report:'\n",
    "        line5 = '\\nConfusion Matrix:'\n",
    "        cm = self.confusion_matrix()\n",
    "        line6 = '\\n'\n",
    "        res_total = line1 + '\\n' + line2 + '\\n' + line3 + '\\n' + line4 + '\\n' + str(result) + '\\n' + line5 + '\\n' + str(cm) + '\\n' + line6\n",
    "        # write to file\n",
    "        if save == True:\n",
    "            with open('model-comparison/{}/{}/report.txt'.format(now,str1), 'w') as f:\n",
    "                f.write(res_total)\n",
    "        if print_result == True:\n",
    "            print(res_total)\n",
    "\n",
    "    def accuracy_score(self, y_t, y_p):\n",
    "        correct = sum(y_t == y_p)\n",
    "        return correct / len(y_t)\n",
    "\n",
    "    def scores(self, y_t, y_p, class_label= 1):\n",
    "        true = y_t == class_label\n",
    "        pred = y_p == class_label\n",
    "        tp = sum(true & pred)\n",
    "        fp = sum(~true & pred) \n",
    "        fn = sum(true & ~pred)\n",
    "        tn = sum(~true & ~pred) \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return precision, recall, f1\n",
    "    \n",
    "    def confusion_matrix(self,labels=None):\n",
    "        labels = labels if labels else sorted(set(self.y_true) | set(self.y_pred))        \n",
    "        indexes = {v:i for i, v in enumerate(labels)}\n",
    "        matrix = np.zeros((len(indexes),len(indexes))).astype(int)\n",
    "        for t, p in zip(self.y_true, self.y_pred):\n",
    "            matrix[indexes[t], indexes[p]] += 1\n",
    "        # print('Confusion Matrix: ')\n",
    "        # print(pd.DataFrame(matrix, index=labels, columns=labels))\n",
    "        return pd.DataFrame(matrix, index=labels, columns=labels)\n",
    "\n",
    "    def classification_report(self):\n",
    "        output_dict = {}\n",
    "        support_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        f1_list = []\n",
    "        for i in np.unique(self.y_true):\n",
    "            support = sum(self.y_true == i)\n",
    "            precision, recall, f1 = self.scores(self.y_true, self.y_pred, class_label=i)\n",
    "            output_dict[i] = {'precision':precision, 'recall':recall, 'f1-score':f1, 'support':support}\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            f1_list.append(f1)\n",
    "            support_list.append(support)\n",
    "        support = np.sum(support_list)\n",
    "        output_dict['accuracy'] = {'precision':0, 'recall':0, 'f1-score':self.accuracy_score(self.y_true, self.y_pred), 'support':support}\n",
    "        # macro avg\n",
    "        macro_precision = np.mean(precision_list)\n",
    "        macro_recall = np.mean(recall_list)\n",
    "        macro_f1 = np.mean(f1_list)\n",
    "        output_dict['macro avg'] = {'precision':macro_precision, 'recall':macro_recall, 'f1-score':macro_f1, 'support':support}\n",
    "        # weighted avg\n",
    "        weighted_precision = np.average(precision_list, weights=support_list)\n",
    "        weighted_recall = np.average(recall_list, weights=support_list)\n",
    "        weighted_f1 = np.average(f1_list, weights=support_list)\n",
    "        output_dict['weighted avg'] = {'precision':weighted_precision, 'recall':weighted_recall, 'f1-score':weighted_f1, 'support':support}\n",
    "        # convert to dataframe and format\n",
    "        report_d = pd.DataFrame(output_dict).T\n",
    "        annot = report_d.copy()\n",
    "        annot.iloc[:, 0:3] = (annot.iloc[:, 0:3]*100).applymap('{:.2f}'.format) + ' %'\n",
    "        annot['support'] = annot['support'].astype(int)\n",
    "        annot.loc['accuracy','precision'] = ''\n",
    "        annot.loc['accuracy','recall'] = ''\n",
    "        return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]iter  1 act 2.263e+03 pre 1.953e+03 delta 8.243e-01 f 3.549e+03 |g| 2.522e+04 CG  11\n",
      "cg reaches trust region boundary\n",
      "iter  2 act 7.600e+02 pre 6.043e+02 delta 1.115e+00 f 1.286e+03 |g| 6.864e+03 CG  12\n",
      "iter  3 act 3.219e+02 pre 2.545e+02 delta 1.213e+00 f 5.257e+02 |g| 2.839e+03 CG  15\n",
      "iter  4 act 1.215e+02 pre 9.614e+01 delta 1.213e+00 f 2.038e+02 |g| 1.137e+03 CG  15\n",
      "iter  5 act 4.353e+01 pre 3.462e+01 delta 1.213e+00 f 8.231e+01 |g| 4.364e+02 CG  14\n",
      "iter  6 act 1.434e+01 pre 1.160e+01 delta 1.213e+00 f 3.878e+01 |g| 1.661e+02 CG  14\n",
      "iter  7 act 3.642e+00 pre 3.038e+00 delta 1.213e+00 f 2.444e+01 |g| 6.196e+01 CG  12\n",
      "iter  8 act 6.704e-01 pre 5.805e-01 delta 1.213e+00 f 2.079e+01 |g| 2.200e+01 CG  13\n",
      "iter  9 act 1.335e-01 pre 1.259e-01 delta 1.213e+00 f 2.012e+01 |g| 6.132e+00 CG  23\n",
      "iter  1 act 2.547e+03 pre 2.208e+03 delta 1.536e+00 f 3.549e+03 |g| 1.296e+04 CG  17\n",
      "iter  2 act 6.326e+02 pre 5.050e+02 delta 1.576e+00 f 1.002e+03 |g| 3.659e+03 CG  22\n",
      "iter  3 act 2.209e+02 pre 1.744e+02 delta 1.576e+00 f 3.693e+02 |g| 1.363e+03 CG  21\n",
      "iter  4 act 8.242e+01 pre 6.545e+01 delta 1.576e+00 f 1.484e+02 |g| 5.217e+02 CG  22\n",
      "iter  5 act 2.770e+01 pre 2.226e+01 delta 1.576e+00 f 6.600e+01 |g| 1.973e+02 CG  21\n",
      "iter  6 act 7.792e+00 pre 6.438e+00 delta 1.576e+00 f 3.830e+01 |g| 7.376e+01 CG  21\n",
      "iter  7 act 1.524e+00 pre 1.302e+00 delta 1.576e+00 f 3.051e+01 |g| 2.631e+01 CG  20\n",
      "iter  8 act 2.907e-01 pre 2.642e-01 delta 1.576e+00 f 2.898e+01 |g| 7.461e+00 CG  33\n",
      "iter  9 act 2.527e-02 pre 2.390e-02 delta 1.576e+00 f 2.869e+01 |g| 1.466e+00 CG  38\n",
      "iter  1 act 2.725e+03 pre 2.379e+03 delta 1.883e+00 f 3.549e+03 |g| 1.346e+04 CG  16\n",
      "iter  2 act 5.252e+02 pre 4.194e+02 delta 1.883e+00 f 8.235e+02 |g| 3.517e+03 CG  16\n",
      "iter  3 act 1.801e+02 pre 1.428e+02 delta 1.883e+00 f 2.983e+02 |g| 1.308e+03 CG  16\n",
      "iter  4 act 6.401e+01 pre 5.100e+01 delta 1.883e+00 f 1.183e+02 |g| 4.922e+02 CG  16\n",
      "iter  5 act 2.073e+01 pre 1.677e+01 delta 1.883e+00 f 5.425e+01 |g| 1.843e+02 CG  15\n",
      "iter  6 act 5.306e+00 pre 4.450e+00 delta 1.883e+00 f 3.352e+01 |g| 6.847e+01 CG  16\n",
      "iter  7 act 8.364e-01 pre 7.296e-01 delta 1.883e+00 f 2.822e+01 |g| 2.412e+01 CG  15\n",
      "iter  8 act 1.520e-01 pre 1.382e-01 delta 1.883e+00 f 2.738e+01 |g| 6.779e+00 CG  27\n",
      "iter  9 act 2.187e-02 pre 2.163e-02 delta 1.883e+00 f 2.723e+01 |g| 1.075e+00 CG  36\n",
      "iter  1 act 2.885e+03 pre 2.546e+03 delta 2.277e+00 f 3.549e+03 |g| 2.918e+03 CG  17\n",
      "iter  2 act 4.349e+02 pre 3.491e+02 delta 2.277e+00 f 6.641e+02 |g| 7.090e+02 CG  16\n",
      "iter  3 act 1.387e+02 pre 1.106e+02 delta 2.277e+00 f 2.292e+02 |g| 2.489e+02 CG  16\n",
      "iter  4 act 4.572e+01 pre 3.669e+01 delta 2.277e+00 f 9.048e+01 |g| 8.935e+01 CG  16\n",
      "iter  5 act 1.326e+01 pre 1.090e+01 delta 2.277e+00 f 4.476e+01 |g| 3.139e+01 CG  15\n",
      "iter  6 act 2.555e+00 pre 2.223e+00 delta 2.277e+00 f 3.150e+01 |g| 1.020e+01 CG  16\n",
      "iter  7 act 1.956e-01 pre 1.841e-01 delta 2.277e+00 f 2.894e+01 |g| 2.690e+00 CG  28\n",
      "iter  8 act 3.294e-03 pre 3.219e-03 delta 2.277e+00 f 2.875e+01 |g| 4.922e-01 CG  32\n",
      "iter  9 act 4.116e-05 pre 4.103e-05 delta 2.277e+00 f 2.874e+01 |g| 5.790e-02 CG  32\n",
      "iter 10 act 9.051e-07 pre 9.051e-07 delta 2.277e+00 f 2.874e+01 |g| 5.335e-03 CG  42\n",
      "Val. Accuracy:  93.75\n",
      "[[308  11   1   0]\n",
      " [ 13 207   4   0]\n",
      " [  2   8  79   0]\n",
      " [  0   1   0   6]]\n",
      "Test Accuracy:  95.78\n",
      "[[308  12   0   0]\n",
      " [  6 216   2   0]\n",
      " [  1   5  84   0]\n",
      " [  0   0   1   5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model1 = linear_model.LogisticRegression(max_iter=100, \n",
    "                                            penalty='l2', \n",
    "                                            solver='liblinear', \n",
    "                                            multi_class='ovr',\n",
    "                                            verbose=1,\n",
    "                                            )\n",
    "history = model1.fit(X_train, y_train)\n",
    "y_pred = model1.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "#Validation Results\n",
    "poly_pred = model1.predict(X_val)\n",
    "poly_accuracy = accuracy_score(y_val, poly_pred)\n",
    "poly_f1 = f1_score(y_val, poly_pred, average='weighted')\n",
    "print('Val. Accuracy: ', \"%.2f\" % (poly_accuracy*100))\n",
    "cm = confusion_matrix(y_val, poly_pred)\n",
    "print(cm)\n",
    "\n",
    "# Test Results\n",
    "poly_pred = model1.predict(X_test)\n",
    "poly_accuracy = accuracy_score(y_test, poly_pred)\n",
    "poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n",
    "print('Test Accuracy: ', \"%.2f\" % (poly_accuracy*100))\n",
    "cm = confusion_matrix(y_test, poly_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]................*........*\n",
      "optimization finished, #iter = 24903\n",
      "obj = -0.342654, rho = -0.205053\n",
      "nSV = 1816, nBSV = 0\n",
      "....*..*\n",
      "optimization finished, #iter = 6921\n",
      "obj = -0.073576, rho = -0.913113\n",
      "nSV = 1023, nBSV = 0\n",
      "*\n",
      "optimization finished, #iter = 897\n",
      "obj = -0.004285, rho = -1.208193\n",
      "nSV = 287, nBSV = 0\n",
      ".....*...*\n",
      "optimization finished, #iter = 8865\n",
      "obj = -0.091238, rho = -0.929936\n",
      "nSV = 1128, nBSV = 0\n",
      "*\n",
      "optimization finished, #iter = 830\n",
      "obj = -0.004497, rho = -1.266181\n",
      "nSV = 276, nBSV = 0\n",
      "*.*\n",
      "optimization finished, #iter = 772\n",
      "obj = -0.003949, rho = -1.026490\n",
      "nSV = 243, nBSV = 0\n",
      "Total nSV = 3164\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "poly = svm.SVC(kernel='linear', degree=3, C=1, verbose=1).fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "#Validation Results\n",
    "poly_pred = poly.predict(X_val)\n",
    "poly_accuracy = accuracy_score(y_val, poly_pred)\n",
    "poly_f1 = f1_score(y_val, poly_pred, average='weighted')\n",
    "print('Val. Accuracy: ', \"%.2f\" % (poly_accuracy*100))\n",
    "cm = confusion_matrix(y_val, poly_pred)\n",
    "print(cm)\n",
    "\n",
    "# Test Results\n",
    "poly_pred = poly.predict(X_test)\n",
    "poly_accuracy = accuracy_score(y_test, poly_pred)\n",
    "poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n",
    "print('Test Accuracy: ', \"%.2f\" % (poly_accuracy*100))\n",
    "cm = confusion_matrix(y_test, poly_pred)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs464",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
