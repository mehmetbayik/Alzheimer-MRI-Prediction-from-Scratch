{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "dependencies:\n",
    "  - python=3.8.17\n",
    "  - numpy=1.24.0\n",
    "  - matplotlib=3.7.1\n",
    "  - pandas=2.0.2 \n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from itertools import product \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# import random\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# np.random.RandomState(42)\n",
    "# os.environ['TF_DETERMINISTIC_OPS'] = '1' \n",
    "\n",
    "finish_sound = \"afplay /Users/mehmet/Documents/vs-code/winsquare.mp3\"\n",
    "# play sound when finished\n",
    "# os.system(finish_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5120, 10859) (5120, 4) \n",
      " (640, 10859) (640, 4) \n",
      " (640, 10859) (640, 4)\n"
     ]
    }
   ],
   "source": [
    "# Read data from npy file ( already preprocessed )\n",
    "filename = 'original-numpy'\n",
    "# filename = 'pca-numpy'\n",
    "X_train = np.load(f'dataset/{filename}/X_train.npy')\n",
    "X_val = np.load(f'dataset/{filename}/X_val.npy')\n",
    "X_test = np.load(f'dataset/{filename}/X_test.npy')\n",
    "y_train = np.load(f'dataset/{filename}/y_train.npy')\n",
    "y_val = np.load(f'dataset/{filename}/y_val.npy')\n",
    "y_test = np.load(f'dataset/{filename}/y_test.npy')\n",
    "print(X_train.shape, y_train.shape,'\\n', X_val.shape, y_val.shape,'\\n', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    # Logistic Regression Model written from scratch without Bias w0\n",
    "    def __init__(self, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.W = None\n",
    "        self.now = None\n",
    "        self.print_result = True\n",
    "        self.history_steps1 = None\n",
    "        self.history = None\n",
    "        self.validation_accuracy = None\n",
    "        \n",
    "    def validation_accuracy(self):\n",
    "        return self.validation_accuracy\n",
    "    \n",
    "    def history(self):\n",
    "        return self.history\n",
    "    \n",
    "    def load_history(self):\n",
    "        pd_hist = pd.read_csv(f'model-comparison/{self.now}/history.csv')\n",
    "        self.history = np.array(pd_hist.iloc[:,1:])\n",
    "        \n",
    "    def plot(self, save = True):\n",
    "        # Save history as csv file\n",
    "        history_local = self.history\n",
    "        if type(history_local) is not pd.DataFrame:\n",
    "            history_df = pd.DataFrame(history_local)\n",
    "        if save == True:\n",
    "            hist_csv_file = f'model-comparison/{self.now}/history.csv'\n",
    "            with open(hist_csv_file, mode='w') as f:\n",
    "                history_df.to_csv(f) \n",
    "        # Plot Loss and Accuracy History as Subplots\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(10, 2)\n",
    "        index = np.arange(1,self.history.shape[1]+1)*self.history_steps1\n",
    "\n",
    "        ax[0].plot(index, self.history[0], label='Training Loss')\n",
    "        ax[0].plot(index, self.history[2], label='Validation Loss')\n",
    "        ax[0].set_title('Loss History')\n",
    "        ax[0].set_xlabel('Epoch')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].legend()\n",
    "        # find best validation accuracy and its epoch\n",
    "        best_val_acc = np.max(self.history[3])  \n",
    "        best_val_acc_epoch = (np.argmax(self.history[3]) + 1)*self.history_steps1\n",
    "        label='Best Epoch = '+str(best_val_acc_epoch)+'\\nVal. Acc. = '+str((best_val_acc*100).round(2))+ '%'\n",
    "        ax[1].plot(index, self.history[1], label='Training Accuracy')\n",
    "        ax[1].plot(index, self.history[3], label='Validation Accuracy')\n",
    "        ax[1].plot(best_val_acc_epoch, best_val_acc, 'ro', label=label)\n",
    "        ax[1].set_title('Accuracy History')\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].set_ylabel('Accuracy')\n",
    "        ax[1].legend()\n",
    "        if save is True and self.now is not None:\n",
    "            plt.savefig(f'model-comparison/{self.now}/plot.png')\n",
    "        if self.print_result == True:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "        \n",
    "    \n",
    "    def validation(self, X_nonbiased, y, W, lmbda):\n",
    "        # add bias\n",
    "        ones=np.ones(X_nonbiased.shape[0])\n",
    "        X=np.c_[ones,X_nonbiased]\n",
    "        # Find loss and accuracy on validation set\n",
    "        y_onehot = y # y is already one-hot encoded\n",
    "        Z = - X @ W\n",
    "        P = np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "        loss = - np.sum(y_onehot * np.log(P)) + lmbda * np.sum(W**2)\n",
    "        y_pred = self.predict(X_nonbiased)\n",
    "        accuracy = np.mean(y_pred == np.argmax(y, axis=1))\n",
    "        return loss, accuracy\n",
    "    \n",
    "    def fit(self, X_nonbiased, y, X_val, y_val, now = None, print_result = True,max_epoch=400, \n",
    "            batch_size=5120, weight_init='zero', lr=0.01, lr_type = 'static', regularization='l2: 0.01', \n",
    "            history_steps = 50, print_step = 100):\n",
    "        start_time = datetime.datetime.now()\n",
    "        # if there isn't model-comparison folder, create it\n",
    "        if not os.path.exists('model-comparison'):\n",
    "            os.mkdir('model-comparison')\n",
    "        self.print_result = print_result\n",
    "        if now is not None:\n",
    "            self.now = now\n",
    "        # Create folder for current model\n",
    "            if not os.path.exists('model-comparison/'+now):\n",
    "                os.mkdir('model-comparison/'+now)\n",
    "\n",
    "        self.history_steps1 = history_steps\n",
    "        self.history = np.zeros((4,max_epoch//history_steps))\n",
    "        y_onehot = y # y is already one-hot encoded\n",
    "        lr_print = str(lr) + ' ' + lr_type\n",
    "        model_specs = 'LR | Batch Size: {} | Weight Init. {} | lr: {} | Regularization: {} | Max Epoch: {} |'.format(batch_size, weight_init, lr_print, regularization, max_epoch)\n",
    "        \n",
    "        # add bias\n",
    "        ones=np.ones(X_nonbiased.shape[0])\n",
    "        X=np.c_[ones,X_nonbiased]\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        # Initialize weights ( shape = features x classes matrix )\n",
    "        if weight_init == 'zero':\n",
    "            self.W = np.zeros((X.shape[1], y_onehot.shape[1])) \n",
    "        elif weight_init == 'uniform':\n",
    "            self.W = np.random.uniform(0, 1, (X.shape[1], y_onehot.shape[1]))\n",
    "        elif weight_init == 'normal':\n",
    "            self.W = np.random.normal(0, 1, (X.shape[1], y_onehot.shape[1]))\n",
    "        \n",
    "        # Print loss and accuracy every 100 iterations or every max_iter//10 iterations if max_iter >= 1000\n",
    "        if max_epoch >= 1000:\n",
    "            print_step = max_epoch // 10\n",
    "\n",
    "        # Gradient Descent\n",
    "        for epoch in range(1, max_epoch+1):\n",
    "            # Shuffle all data X and y in the same order every epoch\n",
    "            shuffle_index = np.arange(X.shape[0])\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            X = X[shuffle_index]\n",
    "            y_onehot = y_onehot[shuffle_index]\n",
    "            \n",
    "            for iteration in range(X.shape[0]//batch_size):                                        \n",
    "                X_batch = X[batch_size*iteration:batch_size*(iteration+1)]\n",
    "                y_batch = y_onehot[batch_size*iteration:batch_size*(iteration+1)]\n",
    "                Z_batch = - X_batch @ self.W\n",
    "                # For numerical stability\n",
    "                ### Z_batch = Z_batch - np.max(Z_batch, axis=1, keepdims=True)\n",
    "                # Logistic function to find probabilities\n",
    "                P_batch = np.exp(Z_batch) / (np.sum(np.exp(Z_batch), axis=1, keepdims=True))                \n",
    "                N_batch = batch_size\n",
    "                # Derivative of Residual ( log-loss )\n",
    "                # P_batch = Softmax(- X_batch @ self.W)\n",
    "                dRSS = (2/N_batch)*(X_batch.T @ (y_batch - P_batch))\n",
    "                # Choose regularization\n",
    "                if regularization[0:2] == 'l2':\n",
    "                    # L2 regularization\n",
    "                    lmbda = float(regularization[4:])\n",
    "                    dRegTerm = 2 * (lmbda) * (N_batch/N) * self.W\n",
    "                    # Bias term is not regularized\n",
    "                    dRegTerm[0] *= 0\n",
    "                    \n",
    "                elif regularization[0:2] == 'l1':\n",
    "                    # L1 regularization\n",
    "                    lmbda = float(regularization[4:])\n",
    "                    dRegTerm = lmbda * np.sign(self.W) \n",
    "                    # Bias term is not regularized\n",
    "                    dRegTerm[0] *= 0\n",
    "                else:\n",
    "                    # No regularization\n",
    "                    lmbda = 0\n",
    "                    dRegTerm = 0\n",
    "                # Calculate gradient\n",
    "                gradient = dRSS + dRegTerm\n",
    "            \n",
    "                if lr_type[0:8] == 'momentum':\n",
    "                    if epoch == 1:\n",
    "                        last_gradient = gradient\n",
    "                    else:\n",
    "                        momentum = float(lr_type[10:])\n",
    "                        gradient = gradient + momentum * last_gradient\n",
    "                        last_gradient = gradient\n",
    "                    \n",
    "                # Update weights \n",
    "                # ( W already has bias term, so we don't need seperate update, \n",
    "                # W is (features+1) x classes matrix: bias is in the first row \n",
    "                # and bias is not regularized ) W = (16864+1) x 10\n",
    "                \n",
    "                self.W = self.W - lr * (N_batch/N) * gradient\n",
    "            \n",
    "                # Change learning rate if lr_type is adaptive\n",
    "                if lr_type == 'adaptive':\n",
    "                    if epoch % 300 == 0:                    \n",
    "                        lr = lr * 0.5\n",
    "                        if print_result == True:\n",
    "                            print('Learning rate changed to', lr)\n",
    "\n",
    "            # Calculate loss and accuracy every 50 epochs:\n",
    "            if epoch % history_steps == 0:\n",
    "                # After each x epoch, calculate loss and accuracy on validation set\n",
    "                Z = - X @ self.W\n",
    "                # Numerical stability\n",
    "                ### Z = Z - np.max(Z, axis=1, keepdims=True)\n",
    "                P = np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "                \n",
    "                loss = - np.sum(y_onehot * np.log(P)) + lmbda * np.sum(self.W**2)\n",
    "                accuracy = np.mean(self.predict(X_nonbiased) == np.argmax(y, axis=1))\n",
    "                val_loss = self.validation(X_val, y_val, self.W, lmbda)[0]\n",
    "                val_acc = self.validation(X_val, y_val, self.W, lmbda)[1]\n",
    "                self.validation_accuracy = val_acc\n",
    "                self.history[:,(epoch//history_steps)-1] = np.array([loss, accuracy, val_loss, val_acc])\n",
    "                \n",
    "                # Print loss and accuracy every 100 epochs\n",
    "                if epoch % print_step == 0:\n",
    "                    line1 = 'Epoch: ' + str(epoch)\n",
    "                    line2 = ' | Loss: ' + str(loss) + ' | Accuracy: ' + str(accuracy)[0:5]\n",
    "                    line3 = ' | Val. Loss: ' + str(val_loss) + ' | Val. Acc: ' + str(val_acc)[0:5]\n",
    "                    # line2 = ' | Loss: ' + str(round(loss)) + ' | Accuracy: ' + str(accuracy)[0:5]\n",
    "                    # line3 = ' | Val. Loss: ' + str(round(val_loss)) + ' | Val. Acc: ' + str(val_acc)[0:5]\n",
    "                    if print_result == True:\n",
    "                        print(line1 + line2 + line3)\n",
    "                    if now is not None:\n",
    "                        with open('model-comparison/{}/log.txt'.format(now), 'a') as f:\n",
    "                            f.write(line1 + line2 + line3 + '\\n')\n",
    "            if epoch == max_epoch:\n",
    "                end_time = datetime.datetime.now()\n",
    "                if print_result == True:\n",
    "                    print('Training finished. Time elapsed:', end_time - start_time, '\\n')\n",
    "                    print('Accuracy: ', str(accuracy)[0:5], 'Val. Accuracy: ', str(val_acc)[0:5])\n",
    "                val_acc_print = str(val_acc*100)+ '00'\n",
    "                if now is not None:\n",
    "                    with open('model-comparison/{}/log.txt'.format(now), 'a') as f:\n",
    "                        write_line = 'Training finished. Time elapsed: ' + str(end_time - start_time) + '\\n'\n",
    "                        f.write(write_line)\n",
    "                    with open('model-comparison/{}/{}-val-acc.txt'.format(now,val_acc_print[0:5]), 'w') as f:\n",
    "                        f.write(model_specs)\n",
    "                    with open('model-comparison/last.txt', 'w') as f:\n",
    "                        f.write(str(now))\n",
    "                \n",
    "    def predict(self, X_nonbiased):\n",
    "        # add bias\n",
    "        ones=np.ones(X_nonbiased.shape[0])\n",
    "        X=np.c_[ones,X_nonbiased]\n",
    "        Z = - X @ self.W\n",
    "        # Logistic function to find probabilities\n",
    "        P = np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "        # Predict class\n",
    "        y = np.argmax(P, axis=1)\n",
    "        return y\n",
    "    def save_weights(self):\n",
    "        # save history steps\n",
    "        with open('model-comparison/{}/history_steps.txt'.format(self.now), 'w') as f:\n",
    "            f.write(str(self.history_steps1))\n",
    "        # save weights (bias included in W)\n",
    "        filename = 'model-comparison/{}/weights.npy'.format(self.now)\n",
    "        np.save(filename, self.W)\n",
    "    def load_weights(self, now):\n",
    "        # load history steps\n",
    "        with open('model-comparison/{}/history_steps.txt'.format(now), 'r') as f:\n",
    "            self.history_steps1 = int(f.read())\n",
    "        # load weights (bias included in W)\n",
    "        filename = 'model-comparison/{}/weights.npy'.format(now)\n",
    "        self.W = np.load(filename)\n",
    "        self.now = now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateModel():\n",
    "    # Class to evaluate model performance, similar to sklearn.metrics ClassificationReport and ConfusionMatrix\n",
    "    def __init__(self, y_true, y_pred, str1, now, save=True, print_result=True):\n",
    "        self.y_true = np.argmax(y_true, axis=1)\n",
    "        self.y_pred = y_pred\n",
    "        if save == True:\n",
    "            os.mkdir('model-comparison/'+now+'/'+str1)\n",
    "            np.savetxt('model-comparison/{}/{}/pred.csv'.format(now,str1), y_pred, delimiter=',', fmt='%d')\n",
    "        \n",
    "        result = self.classification_report()\n",
    "        fpr0 = 100 - float(result['precision'][0][0:4])\n",
    "        line1 = 'Accuracy is: ' + str(result['f1-score']['accuracy'])\n",
    "        line2 = 'F1 Score is: ' + str(result['f1-score']['weighted avg'])\n",
    "        line3 = 'Precision of Class 0 is: ' + '{0:.2f}'.format(100-fpr0)+ ' %'\n",
    "        line4 = '\\nClassification Report:'\n",
    "        line5 = '\\nConfusion Matrix:'\n",
    "        cm = self.confusion_matrix()\n",
    "        line6 = '\\n'\n",
    "        res_total = line1 + '\\n' + line2 + '\\n' + line3 + '\\n' + line4 + '\\n' + str(result) + '\\n' + line5 + '\\n' + str(cm) + '\\n' + line6\n",
    "        # write to file\n",
    "        if save == True:\n",
    "            with open('model-comparison/{}/{}/report.txt'.format(now,str1), 'w') as f:\n",
    "                f.write(res_total)\n",
    "        if print_result == True:\n",
    "            print(res_total)\n",
    "\n",
    "    def accuracy_score(self, y_t, y_p):\n",
    "        correct = sum(y_t == y_p)\n",
    "        return correct / len(y_t)\n",
    "\n",
    "    def scores(self, y_t, y_p, class_label= 1):\n",
    "        true = y_t == class_label\n",
    "        pred = y_p == class_label\n",
    "        tp = sum(true & pred)\n",
    "        fp = sum(~true & pred) \n",
    "        fn = sum(true & ~pred)\n",
    "        tn = sum(~true & ~pred) \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return precision, recall, f1\n",
    "    \n",
    "    def confusion_matrix(self,labels=None):\n",
    "        labels = labels if labels else sorted(set(self.y_true) | set(self.y_pred))        \n",
    "        indexes = {v:i for i, v in enumerate(labels)}\n",
    "        matrix = np.zeros((len(indexes),len(indexes))).astype(int)\n",
    "        for t, p in zip(self.y_true, self.y_pred):\n",
    "            matrix[indexes[t], indexes[p]] += 1\n",
    "        # print('Confusion Matrix: ')\n",
    "        # print(pd.DataFrame(matrix, index=labels, columns=labels))\n",
    "        return pd.DataFrame(matrix, index=labels, columns=labels)\n",
    "\n",
    "    def classification_report(self):\n",
    "        output_dict = {}\n",
    "        support_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        f1_list = []\n",
    "        for i in np.unique(self.y_true):\n",
    "            support = sum(self.y_true == i)\n",
    "            precision, recall, f1 = self.scores(self.y_true, self.y_pred, class_label=i)\n",
    "            output_dict[i] = {'precision':precision, 'recall':recall, 'f1-score':f1, 'support':support}\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            f1_list.append(f1)\n",
    "            support_list.append(support)\n",
    "        support = np.sum(support_list)\n",
    "        output_dict['accuracy'] = {'precision':0, 'recall':0, 'f1-score':self.accuracy_score(self.y_true, self.y_pred), 'support':support}\n",
    "        # macro avg\n",
    "        macro_precision = np.mean(precision_list)\n",
    "        macro_recall = np.mean(recall_list)\n",
    "        macro_f1 = np.mean(f1_list)\n",
    "        output_dict['macro avg'] = {'precision':macro_precision, 'recall':macro_recall, 'f1-score':macro_f1, 'support':support}\n",
    "        # weighted avg\n",
    "        weighted_precision = np.average(precision_list, weights=support_list)\n",
    "        weighted_recall = np.average(recall_list, weights=support_list)\n",
    "        weighted_f1 = np.average(f1_list, weights=support_list)\n",
    "        output_dict['weighted avg'] = {'precision':weighted_precision, 'recall':weighted_recall, 'f1-score':weighted_f1, 'support':support}\n",
    "        # convert to dataframe and format\n",
    "        report_d = pd.DataFrame(output_dict).T\n",
    "        annot = report_d.copy()\n",
    "        annot.iloc[:, 0:3] = (annot.iloc[:, 0:3]*100).applymap('{:.2f}'.format) + ' %'\n",
    "        annot['support'] = annot['support'].astype(int)\n",
    "        annot.loc['accuracy','precision'] = ''\n",
    "        annot.loc['accuracy','recall'] = ''\n",
    "        return annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(model_options, X_train, y_train, X_val, y_val, X_test, y_test, print_result=False, seed=42, history_steps=1):\n",
    "    # Grid Search Function\n",
    "    best_metric = 0\n",
    "    for i in range(len(model_options)):\n",
    "        models = model_options[i]\n",
    "        model_number = i + 1\n",
    "        now = datetime.datetime.now().strftime(\"%d-%m-%H-%M\")\n",
    "        # Create folder for current model\n",
    "        if not os.path.exists('model-comparison/'+now):\n",
    "            os.mkdir('model-comparison/'+now)\n",
    "        else:\n",
    "            now = now + str('--1')\n",
    "            os.mkdir('model-comparison/'+now)\n",
    "        model = LogisticRegression(seed=seed)\n",
    "        start_time = datetime.datetime.now()\n",
    "        model.fit(X_train, y_train, X_val, y_val, now, print_result=print_result, max_epoch=models[0], history_steps=history_steps,\n",
    "                  weight_init= models[1], batch_size=models[2], lr=models[3], lr_type=models[4], regularization=models[5])\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_elapsed = str(end_time - start_time)[2:7]\n",
    "        metric = model.validation_accuracy\n",
    "        model.save_weights()\n",
    "        model.plot()\n",
    "        y_pred = model.predict(X_val)\n",
    "        results = EvaluateModel(y_val, y_pred, 'val', now, print_result=print_result)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results = EvaluateModel(y_test, y_pred, 'test', now, print_result=print_result)\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            best_model = now\n",
    "        print('Model ', str(model_number), ' saved with name: ', now)\n",
    "        print(models, 'Val-Accuracy:', metric)\n",
    "\n",
    "        # append to txt file\n",
    "        lr_print = str(models[3]) + ' ' + models[4]\n",
    "        model_specs = 'LR | Batch Size: {} | Weight Init: {} | lr: {} | Regularization: {} | Max Epoch: {}'.format(models[2], models[1], lr_print, models[5], models[0])\n",
    "        with open('model-comparison/best-models.txt', 'a') as f:\n",
    "            f.write(now + ' | ' + model_specs + ' | ' + str(metric) + ' | Time Elapsed: '+ time_elapsed +'\\n')\n",
    "        print(len(model_options)-model_number, 'models left to train.')\n",
    "    best_metric = str(best_metric*100)[:5]\n",
    "    print('Best Model is:', best_model, 'with validation accuracy:', best_metric, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(max_epoch, batch_size, weight_init, lr, lr_type, regularization):\n",
    "    model_options = [[max_epoch, weight_init, batch_size, lr, lr_type, regularization]]\n",
    "    return model_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train New Model\n",
    "model_parameters = TrainModel(\n",
    "    max_epoch=1000, batch_size=512, weight_init='zero', lr=0.01, lr_type='momentum: 0.99', regularization='l2: 0.01')\n",
    "\n",
    "GridSearch(model_parameters, X_train, y_train, X_val, y_val, X_test, y_test, print_result=True, seed=42, history_steps=1)\n",
    "\n",
    "os.system(finish_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations: 486\n",
      "Combination 1: (1000, 'zero', 512, 0.01, 'momentum: 0.99', 'l2: 0.01')\n"
     ]
    }
   ],
   "source": [
    "# Grid Search Combinations\n",
    "\n",
    "max_epoch = [1000]\n",
    "weight_init = ['zero', 'uniform', 'normal']\n",
    "batch_size = [512, 5120, 1]\n",
    "lr = [0.01, 0.005, 0.001]\n",
    "lr_type = ['momentum: 0.99', 'static', 'adaptive']\n",
    "regularization = ['l2: 0.01', 'l2: 0.001', 'l2: 0.0001', 'l1: 0.01', 'l1: 0.001', 'l1: 0.0001']\n",
    "params = [max_epoch, weight_init, batch_size, lr, lr_type, regularization]\n",
    "model_options = list(product(*params))\n",
    "print('Number of combinations:', len(model_options))\n",
    "print('Combination 1:', model_options[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  1  saved with name:  20-12-07-12\n",
      "(1000, 'zero', 512, 0.01, 'momentum: 0.99', 'l2: 0.01') Val-Accuracy: 0.9328125\n",
      "0 models left to train.\n",
      "Best Model is: 20-12-07-12 with validation accuracy: 93.28 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearch(model_options[0:1], X_train, y_train, X_val, y_val, X_test, y_test, seed=42, history_steps=100)\n",
    "os.system(finish_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train new model\n",
    "# now = datetime.datetime.now().strftime(\"%d-%m-%H-%M\")\n",
    "\n",
    "# # Fit model \n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train, X_val, y_val, now, max_epoch=1000,\n",
    "#           batch_size=5120, weight_init='zero', lr=0.01, lr_type='momentum: 0.99', regularization='l2: 0.01')\n",
    "# model.save_weights()\n",
    "# model.plot()\n",
    "\n",
    "# # Validation Set Results\n",
    "# y_pred = model.predict(X_val)\n",
    "# results = EvaluateModel(y_val, y_pred, 'val', now)\n",
    "\n",
    "# # Test Set Results\n",
    "# y_pred = model.predict(X_test)\n",
    "# results = EvaluateModel(y_test, y_pred, 'test', now)\n",
    "\n",
    "# # play sound when finished\n",
    "# os.system(finish_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Trained Model and Evaluate\n",
    "\n",
    "# #now = open('model-comparison/last.txt', 'r').read()\n",
    "# now = '20-12-07-12'\n",
    "# model = LogisticRegression()\n",
    "# model.load_weights(now)\n",
    "# model.load_history()\n",
    "\n",
    "# # Validation Set Results\n",
    "# model.plot(save=False)\n",
    "# y_pred = model.predict(X_val)\n",
    "# results = EvaluateModel(y_val, y_pred, 'val', now, save=False)\n",
    "\n",
    "# # Test Set Results\n",
    "# y_pred = model.predict(X_test)\n",
    "# results = EvaluateModel(y_test, y_pred, 'test', now, save=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs464",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
